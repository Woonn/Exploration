{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "timely-vision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-texas",
   "metadata": {},
   "source": [
    "### BERT에 사용되는 [MASK], [SEP], [CLS] 등의 주요 특수문자가 vocab에 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "prefix = 'ko_32000'\n",
    "vocab_size = 32000\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰\n",
    "\n",
    "print(\"완료=3\")   # 완료메시지가 출력될 때까지 아무 출력내용이 없더라도 기다려 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-captain",
   "metadata": {},
   "source": [
    "### 토크나이저 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_32000.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-bulgarian",
   "metadata": {},
   "source": [
    "### 토크나이저 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수 token 7개를 제외한 나머지 tokens 들\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "    if not vocab.is_unknown(id):\n",
    "        vocab_list.append(vocab.id_to_piece(id))\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-collectible",
   "metadata": {},
   "source": [
    "* SentencePiece 모델을 이용해 간단한 BERT의 Masked Language Model 학습용 데이터를 하나 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CLS], tokens a, [SEP], tokens b, [SEP] 형태의 token 생성\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-assembly",
   "metadata": {},
   "source": [
    "* BERT의 Masked Language Model은 GPT의 Next Token Prediction 태스크처럼 [다음이 이어질 단어는?] 을 맞추는 게 아니라 마스킹된 [다음 빈칸에 알맞은 단어는?] 문제를 푸는 형식으로 구성됩니다. 이런 빈칸은 전체 토큰의 15% 정도가 적당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_org)\n",
    "print(len(tokens_org))\n",
    "print(tokens_org[0])\n",
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-monaco",
   "metadata": {},
   "source": [
    "* 15%를 마스킹한다고 해도 생각해 볼 것이 더 있습니다. Subword 기반으로 토크나이징을 했을 때 _대, [MASK], 민국이라고 가운데를 마스킹했을 경우 해당 [MASK]가 '한'일 거라는 건 너무 쉽게 맞출 수 있습니다. '대한민국'이라는 패턴을 아주 자주 보게 될 테니까요.\n",
    "\n",
    "* 그래서 Masked LM 태스크를 구성할 땐 띄어쓰기 단위로 한꺼번에 마스킹해 주는 것이 좋습니다. 다음과 같이 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u\"\\u2581\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기 단위로 mask 하기 위해서 index 분할\n",
    "cand_idx = []  # word 단위의 index array\n",
    "\n",
    "for (i, token) in enumerate(tokens_org):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue# 종료시점\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):# 첫 숫자 입력 후 쭉, 밑줄시작 아니면\n",
    "        cand_idx[-1].append(i)# [] 안에 추가\n",
    "    else:\n",
    "        cand_idx.append([i])# 새로운 시작점\n",
    "\n",
    "# 결과확인\n",
    "for cand in cand_idx:\n",
    "    print(cand, [tokens_org[i] for i in cand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random mask를 위해서 순서를 섞음\n",
    "random.shuffle(cand_idx)\n",
    "cand_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-linux",
   "metadata": {},
   "source": [
    "* 개선된 Masking 로직을 다음과 같이 구현해 보았습니다. 마스킹 된 결과를 이전과 비교해 보시죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "mask_lms = []  # mask 된 값\n",
    "for index_set in cand_idx:\n",
    "    if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "          break\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "          continue\n",
    "    dice = random.random()  # 0..1 사이의 확률 값\n",
    "\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "        if dice < 0.8:  # 80% replace with [MASK]\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9: # 10% keep original\n",
    "            masked_token = tokens[index]\n",
    "        else:  # 10% random word\n",
    "            masked_token = random.choice(vocab_list)\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "        tokens[index] = masked_token\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-application",
   "metadata": {},
   "source": [
    "* Masked LM의 라벨 데이터도 아래와 같이 생성하여 정리해 둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순서 정렬 및 mask_idx, mask_label 생성\n",
    "mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-narrative",
   "metadata": {},
   "source": [
    "* 🔶 create_pretrain_mask() : Masked LM을 위한 코퍼스 생성 메소드\n",
    "이번 스텝에서 구현할 최종 메소드는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0..1 사이의 확률 값\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-digest",
   "metadata": {},
   "source": [
    "* create_pretrain_mask() 수행 결과를 다시 한번 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-cannon",
   "metadata": {},
   "source": [
    "### 16-4. 데이터 전처리 (2) NSP pair 생성\n",
    "* BERT의 pretrain task로 Next Sentence Prediction이 있습니다. 문장 2개를 붙여 놓고 두 문장이 이어지는 것인지 아닌지 문장 호응관계를 맞출 수 있게 하는 것입니다. 아래 문장을 예시로 진행해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        #######################################\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "          \n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-blackberry",
   "metadata": {},
   "source": [
    "* 짝지은 두 문장을 그대로 두면 NSP task의 true label 케이스가 되고, 둘의 순서를 뒤바꾸면 false label 케이스가 되겠죠? 두 문장의 최대 길이를 유지하도록 trim을 적용한 후 50%의 확률로 true/false 케이스를 생성해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        #######################################\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-weather",
   "metadata": {},
   "source": [
    "* 이제 두 문장 사이에 segment 처리를 해주어야 합니다. 첫 번째 문장의 segment는 모두 0으로, 두 번째 문장은 1로 채워준 후 둘 사이에 구분자인 [SEP] 등을 넣어주는 것으로 마무리됩니다.\n",
    "\n",
    "\n",
    "* 이전 스텝의 create_pretrain_mask()까지 함께 호출되어 Mask LM용 데이터셋과 NSP용 데이터셋이 결합된 하나의 데이터셋으로 완성될 것입니다. BERT의 pretrain 은 두가지 task가 동시에 수행되니까요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = []\n",
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        # tokens & aegment 생성\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "        segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "        print(\"tokens:\", len(tokens), tokens)\n",
    "        print(\"segment:\", len(segment), segment)\n",
    "        # mask\n",
    "        tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "        print(\"masked tokens:\", len(tokens), tokens)\n",
    "        print(\"masked index:\", len(mask_idx), mask_idx)\n",
    "        print(\"masked label:\", len(mask_label), mask_label)\n",
    "\n",
    "        instance = {\n",
    "            \"tokens\": tokens,\n",
    "            \"segment\": segment,\n",
    "            \"is_next\": is_next,\n",
    "            \"mask_idx\": mask_idx,\n",
    "            \"mask_label\": mask_label\n",
    "        }\n",
    "        instances.append(instance)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-viewer",
   "metadata": {},
   "source": [
    "* 🔶 create_pretrain_instances() : Next Sentence Prediction을 위한 코퍼스 생성 메소드\n",
    "\n",
    "* 이번 스텝에서 구현할 최종 메소드는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line\n",
    "        current_length += len(doc[i])\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-reservation",
   "metadata": {},
   "source": [
    "* create_pretrain_instances() 수행 결과를 다시 한번 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-floor",
   "metadata": {},
   "source": [
    "## 16-5. 데이터 전처리 (3) 데이터셋 완성\n",
    "\n",
    "* 이제 우리가 다루어야 할 kowiki.txt에 대해 본격적으로 들여다보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-definition",
   "metadata": {},
   "source": [
    "* 전체 라인 수가 확인되시나요? 거의 400만 개에 육박하는 수치입니다.\n",
    "\n",
    "* 위키 문서는 하나의 도큐먼트가 주제 키워드에 대해 상세 내용이 설명으로 따라붙어 있는 형태로 구성되어 있지요? 도큐먼트 주제별로 잘 나눠지는지도 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "            if 0 < len(doc):\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break\n",
    "                doc = []\n",
    "        else:  # doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line)\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-bridge",
   "metadata": {},
   "source": [
    "* 이전 스텝에서 완성했던 create_pretrain_instances()를 코퍼스에 적용할 수 있는지 몇 라인에 대해서만 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance 생성 기능 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # save\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                print(instances[0])\n",
    "                print(instances[-1])\n",
    "                print()\n",
    "                doc = []\n",
    "                if 0 < count:  # 테스트를 위해서 부분 처리 함\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:  # doc에 저장\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-metro",
   "metadata": {},
   "source": [
    "* 🔶 make_pretrain_data() : BERT pretrain 데이터셋 생성 메소드\n",
    "* 전체 전처리 과정을 거쳐 최종적으로 만들어지는 BERT pretrain 데이터셋 생성 메소드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-thailand",
   "metadata": {},
   "source": [
    "* 이제 약 400만 라인에 해당하는 전체 코퍼스에 대해 make_pretrain_data()를 구동해 봅시다. 10여 분가량 시간이 소요될 수 있습니다.\n",
    "* 최종적으로 생성된 데이터셋은 json 포맷으로 저장될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-appearance",
   "metadata": {},
   "source": [
    "* 데이터셋 파일을 만드는 것까지 수행되었습니다.\n",
    "\n",
    "* 하지만 여기서 고려해야 할 점이 있습니다. 우리가 다루어야 할 데이터셋은 사이즈가 큽니다. 만들어질 json 데이터파일의 크기가 1.4GB 정도 됩니다. 실제 BERT 학습용dml 백 분의 일 사이즈 정도밖에 안 되겠지만 그럼에도 불구하고 이렇게 큰 파일을 로딩하는 함수를 만들 때는 메모리 사용량과 관련해 고려해야 할 점이 있습니다.\n",
    "\n",
    "* 그래서 우리는 np.memmap을 사용해서 메모리 사용량을 최소화하는 방법을 시도해 볼 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seq = 128\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq - 3\n",
    "\n",
    "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "# enc_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# dec_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# labels_nsp = np.zeros((total,), np.int32)\n",
    "# labels_mlm = np.zeros((total, n_seq), np.int32)\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-fraud",
   "metadata": {},
   "source": [
    "만들어진 json 파일을 라인 단위로 읽어 들여 np.memmap에 로딩해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-young",
   "metadata": {},
   "source": [
    "## 🔶 load_pre_train_data() : 학습에 필요한 데이터를 로딩하는 함수\n",
    "* np.memmap을 사용해 메모리 효율적으로 만들어진 데이터를 로딩하는 함수를 아래와 같이 구성하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-equipment",
   "metadata": {},
   "source": [
    "## 16-6. BERT 모델 구현\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-indicator",
   "metadata": {},
   "source": [
    "* 이제 본격적으로 BERT model을 구현해 보겠습니다.\n",
    "\n",
    "* BERT가 transformer encoder로 구현되어 있다는 것은 잘 알고 계시리라 생각합니다. 이미 여러 번 다뤄보셨을 transformer의 모델구조와 거의 유사하지만, 아래 그림과 같이 3개의 embedding 레이어를 가진다는 점에 유의해야 합니다."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "familiar-discretion",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-discovery",
   "metadata": {},
   "source": [
    "이제 본격적으로 embedding 레이어를 쌓아나가겠습니다. 아래는 Token Embedding의 구현입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-concrete",
   "metadata": {},
   "source": [
    "Positional Embedding 레이어는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: positional embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-windsor",
   "metadata": {},
   "source": [
    "상대적으로 매우 간단한 Segment Embedding은 별도의 레이어를 구현하지 않고 BERT 클래스에서 간단히 포함하도록 하겠습니다.\n",
    "\n",
    "아래는 자주 보았던 ScaleDotProductAttention과 이를 활용한 MultiHeadAttention입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-munich",
   "metadata": {},
   "source": [
    "이를 바탕으로 transformer encoder 레이어를 구성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-herald",
   "metadata": {},
   "source": [
    "이제 다 왔습니다.\n",
    "\n",
    "최종적으로 구성할 BERT 레이어는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionalEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-remedy",
   "metadata": {},
   "source": [
    "BERT 레이어를 바탕으로 최종적으로 만들어질 pretrain용 BERT 모델 구성은 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-hollywood",
   "metadata": {},
   "source": [
    "아주 작은 pretrain용 BERT 모델(test_model)을 생성하여 동작을 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-retrieval",
   "metadata": {},
   "source": [
    "test_model.fit()이 잘 구동되나요?\n",
    "\n",
    "다음 스텝에서 본격적으로 학습을 진행해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-pilot",
   "metadata": {},
   "source": [
    "## 16-7. pretrain 진행\n",
    "\n",
    "loss나 accuracy같이 기본적으로 필요한 계산 함수를 미리 정의해 둡시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-tribe",
   "metadata": {},
   "source": [
    "Learning Rate 스케줄링도 아래와 같이 구현합니다. WarmUp 이후 consine 형태로 감소하는 스케줄을 적용합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-candle",
   "metadata": {},
   "source": [
    "이제 본격적으로 학습을 진행합니다. 1Epoch만 학습하는 데도 10분 이상의 상당한 시간이 소요될 것입니다. 그리고 메모리 오류가 날 수 있으니 배치 사이즈에도 유의해 주세요. 참고로 우리는 전체 데이터셋 중의 1/7 수준인 128000건만 로딩해서 사용 중이라는 것을 기억합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-checkout",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
