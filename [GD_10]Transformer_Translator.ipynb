{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-american",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-eclipse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "analyzed-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import seaborn # Attention 시각화를 위해 필요!\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "changed-release",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-treat",
   "metadata": {},
   "source": [
    "### 포지셔널 인코딩\n",
    "* 임베딩까지 한번에 처리\n",
    "* 인코딩은 단순 구분을 위한 수치화, 임베딩은 단어에 의미를 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-barcelona",
   "metadata": {},
   "source": [
    "* 입력 데이터 → [ batch_size x length ]\n",
    "* Source & Target Embedding → [ batch_size x length x d_emb ]\n",
    "* Positional Encoding 강의 노드에서 구현을 했었죠? 2번의 결과에 더해지므로 shape 변화는 없습니다.\n",
    "* Multi-Head Attention 아래와 같이 여러 개의 서브 모듈들이 존재합니다.\n",
    "* Split Heads → [ batch_size x length x heads x (d_emb / n_heads) ]\n",
    "* Masking for Masked Attention\n",
    "* Scaled Dot Product Attention\n",
    "* Combine Heads →[ batch_size x length x d_emb ]\n",
    "* esidual Connection\n",
    "* Layer Normalization\n",
    "* Position-wise Feed-Forward Network → [ batch_size x length x d_ff ]\n",
    "* Output Linear Layer → [ batch_size x length x vocab_size ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vocational-error",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model) # Linear Layer\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "\n",
    "        \"\"\"\n",
    "        Scaled QK 값 구하기\n",
    "        \"\"\"\n",
    "        # tf.matmul을 사용하는 이유는 입력값으로 다양한 tenser와 None이 들어왔을때를 위해\n",
    "        # tf.math.sqrt를 사용하는 이유는 다양한 tenser를 받기 위해\n",
    "        scaled_qk = tf.matmul(Q, K, transpose_b = True) / tf.math.sqrt(d_k)\n",
    "        \n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9) \n",
    "\n",
    "        \"\"\"\n",
    "        1. Attention Weights 값 구하기 -> attentions\n",
    "        2. Attention 값을 V에 곱하기 -> out\n",
    "        \"\"\" \n",
    "        #axis는 디폴트로 -1, 마지막 차원을 명시해주는것\n",
    "        attestions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Embedding을 Head의 수로 분할하는 함수\n",
    "\n",
    "        x: [ batch x length x emb ]\n",
    "        return: [ batch x length x heads x self.depth ]\n",
    "        \"\"\"\n",
    "        def split_heads(self, x):\n",
    "            bsz = x.shape[0]# 일반적으로 입력텐서는 (batch_size, max_len, embedding_dim으로 구성)\n",
    "            split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))# reshape(tensor, 변환형태), -1은 구조를 맞춰줌\n",
    "            split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])# perm: 입력 텐서의 인덱스를 기준으로 변환\n",
    "\n",
    "        return split_x\n",
    "\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 Embedding을 하나로 결합하는 함수\n",
    "\n",
    "        x: [ batch x length x heads x self.depth ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "\n",
    "        return combined_x\n",
    "    \n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        아래 순서에 따라 소스를 작성하세요.\n",
    "\n",
    "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
    "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
    "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
    "                 -> out, attention_weights\n",
    "        Step 4: Combine Heads(out) -> out\n",
    "        Step 5: Linear_out(out) -> out\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return out, attention_weights\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accurate-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)# [1, 100, 300]\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-platinum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wireless-affair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"', '모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하지 않는다.', '그러나 이것은 또한 책상도 필요로 하지 않는다.', '79.95달러하는 이 최첨단 무선 광마우스는 허공에서 팔목, 팔, 그외에 어떤 부분이든 그 움직임에따라 커서의 움직임을 조절하는 회전 운동 센서를 사용하고 있다.', '정보 관리들은 동남 아시아에서의 선박들에 대한 많은 (테러) 계획들이 실패로 돌아갔음을 밝혔으며, 세계 해상 교역량의 거의 3분의 1을 운송하는 좁은 해로인 말라카 해협이 테러 공격을 당하기 쉽다고 경고하고 있다.'] ['Much of personal computing is about \"can you top this?\"', 'so a mention a few weeks ago about a rechargeable wireless optical mouse brought in another rechargeable, wireless mouse.', \"Like all optical mice, But it also doesn't need a desk.\", 'uses gyroscopic sensors to control the cursor movement as you move your wrist, arm, whatever through the air.', \"Intelligence officials have revealed a spate of foiled plots on ships in Southeast Asia and are warning that a narrow stretch of water carrying almost one third of the world's maritime trade is vulnerable to a terror attack.\"]\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "    \n",
    "    check_point = {}\n",
    "    kor_cleaned_corpus = []\n",
    "    eng_cleaned_corpus = []\n",
    "\n",
    "    for k, e in zip(kor, eng):\n",
    "        if k not in check_point:\n",
    "            check_point[k] = 1\n",
    "            kor_cleaned_corpus.append(k)\n",
    "            eng_cleaned_corpus.append(e)\n",
    "\n",
    "            \n",
    "    return kor_cleaned_corpus, eng_cleaned_corpus\n",
    "\n",
    "k_corpus, e_corpus = clean_corpus(kor_path, eng_path)\n",
    "print(k_corpus[:5], e_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unsigned-indie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77591"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(k_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-purse",
   "metadata": {},
   "source": [
    "2) 정제 함수를 아래 조건을 만족하게 정의하세요.\n",
    "\n",
    "조건\n",
    "\n",
    "* 모든 입력을 소문자로 변환합니다.\n",
    "* 알파벳, 문장부호, 한글만 남기고 모두 제거합니다.\n",
    "* 문장부호 양옆에 공백을 추가합니다.\n",
    "* 문장 앞뒤의 불필요한 공백을 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ruled-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([!,.?])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z!,.?]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-arrest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "international-sudan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaa에이에이에이에이 오늘도 힘들다 !'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(\"AAA에이에이에이에이 오늘도 힘들다 1 2  3 ! @   #\", s_token = True, e_token = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-punishment",
   "metadata": {},
   "source": [
    "3) 한글 말뭉치 kor_corpus 와 영문 말뭉치 eng_corpus 를 각각 분리한 후, 정제하여 토큰화를 진행합니다! 토큰화에는 Sentencepiece를 활용하세요. 첨부된 공식 사이트를 참고해 아래 조건을 만족하는 generate_tokenizer() 함수를 정의합니다. 최종적으로 ko_tokenizer 과 en_tokenizer 를 얻으세요. en_tokenizer에는 set_encode_extra_options(\"bos:eos\") 함수를 실행해 타겟 입력이 문장의 시작 토큰과 끝 토큰을 포함할 수 있게 합니다.\n",
    "\n",
    "조건\n",
    "\n",
    "* 단어 사전을 매개변수로 받아 원하는 크기의 사전을 정의할 수 있게 합니다. (기본: 20,000)\n",
    "* 학습 후 저장된 model 파일을 SentencePieceProcessor() 클래스에 Load()한 후 반환합니다.\n",
    "* 특수 토큰의 인덱스를 아래와 동일하게 지정합니다.\n",
    "  * <PAD> : 0 / <BOS> : 1 / <EOS> : 2 / <UNK> : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abandoned-declaration",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n",
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang,\n",
    "                       pad_id=0,\n",
    "                       bos_id=1,\n",
    "                       eos_id=2,\n",
    "                       unk_id=3):\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write('{}\\n'.format(row))\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer\n",
    "    \n",
    "kor_corpus = []\n",
    "eng_corpus = []\n",
    "\n",
    "for k, e in zip(k_corpus, e_corpus):\n",
    "    \n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e, s_token = True, e_token = True))\n",
    "    \n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 10000\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adaptive-prospect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77591\n",
      "['개인용 컴퓨터 사용의 상당 부분은 이것보다 뛰어날 수 있느냐 ?', '모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하지 않는다 .', '그러나 이것은 또한 책상도 필요로 하지 않는다 .', '. 달러하는 이 최첨단 무선 광마우스는 허공에서 팔목 , 팔 , 그외에 어떤 부분이든 그 움직임에따라 커서의 움직임을 조절하는 회전 운동 센서를 사용하고 있다 .', '정보 관리들은 동남 아시아에서의 선박들에 대한 많은 테러 계획들이 실패로 돌아갔음을 밝혔으며 , 세계 해상 교역량의 거의 분의 을 운송하는 좁은 해로인 말라카 해협이 테러 공격을 당하기 쉽다고 경고하고 있다 .'] ['much of personal computing is about can you top this ?', 'so a mention a few weeks ago about a rechargeable wireless optical mouse brought in another rechargeable , wireless mouse .', 'like all optical mice , but it also doesn t need a desk .', 'uses gyroscopic sensors to control the cursor movement as you move your wrist , arm , whatever through the air .', 'intelligence officials have revealed a spate of foiled plots on ships in southeast asia and are warning that a narrow stretch of water carrying almost one third of the world s maritime trade is vulnerable to a terror attack .']\n"
     ]
    }
   ],
   "source": [
    "print(len(kor_corpus))\n",
    "print(kor_corpus[:5], eng_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "utility-commander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그러나 이것은 또한 책상도 필요로 하지 않는다 .\n"
     ]
    }
   ],
   "source": [
    "print(kor_corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-endorsement",
   "metadata": {},
   "source": [
    "### 텍스트를 토큰화하는 2가지 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "moderate-tennessee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "correct-shell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어단위로 토큰화\n",
    "ko_tokenizer.piece_to_id(kor_corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "precise-entertainment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 2645,\n",
       " 3780,\n",
       " 6169,\n",
       " 2391,\n",
       " 6828,\n",
       " 6389,\n",
       " 353,\n",
       " 4598,\n",
       " 0,\n",
       " 13,\n",
       " 1012,\n",
       " 2333,\n",
       " 4308,\n",
       " 0,\n",
       " 550,\n",
       " 54,\n",
       " 432,\n",
       " 4249,\n",
       " 880,\n",
       " 671,\n",
       " 1179,\n",
       " 0,\n",
       " 2052,\n",
       " 461,\n",
       " 332,\n",
       " 2273,\n",
       " 54,\n",
       " 633,\n",
       " 4]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 전체를 토큰화\n",
    "ko_tokenizer.EncodeAsIds(eng_corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cognitive-award",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj62/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e223597e314938b81711d5b4660a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다. \n",
    "for i in tqdm_notebook(range(len(kor_corpus))):# for문 진행상황을 보고싶을 때\n",
    "    # [[YOUR CODE]]\n",
    "    if len(kor_corpus[i])<=50:\n",
    "        kor = ko_tokenizer.EncodeAsIds(kor_corpus[i])\n",
    "        eng = en_tokenizer.EncodeAsIds(eng_corpus[i])\n",
    "        \n",
    "        src_corpus.append(kor)\n",
    "        tgt_corpus.append(eng)\n",
    "    \n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='pre', maxlen = 50)\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='pre', maxlen = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "serious-amsterdam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0 1242  214  689  599    6 1571 1316    8 1464\n",
      "  276 2099  894   30   99 4264    0  574]\n",
      "(25689, 50) (25689, 50)\n"
     ]
    }
   ],
   "source": [
    "print(enc_train[0])\n",
    "print(enc_train.shape, dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-diary",
   "metadata": {},
   "source": [
    "## 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-numbers",
   "metadata": {},
   "source": [
    "### positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "familiar-satin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-fabric",
   "metadata": {},
   "source": [
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "endless-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-affairs",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "white-quick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-tonight",
   "metadata": {},
   "source": [
    "### Encoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fatal-animation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-wheel",
   "metadata": {},
   "source": [
    "### Decoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "paperback-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sticky-proceeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "absent-dance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-master",
   "metadata": {},
   "source": [
    "### Transformer 완성하기\n",
    "조건\n",
    "\n",
    "* shared 변수를 매개변수로 받아 True 일 경우 Decoder Embedding과 출력층 Linear의 Weight를 공유할 수 있게 하세요! Weight가 공유될 경우 Embedding 값에 sqrt(d_model)을 곱해줘야 하는 것, 잊지 않으셨죠? (참고: tf.keras.layers.Layer.set_weights())\n",
    "* 우리가 정의한 positional_encoding 의 반환값 형태는 [ Length x d_model ] 인데, 이를 더해 줄 Embedding 값 형태가 [ Batch x Length x d_model ] 이라서 연산이 불가능합니다. 연산이 가능하도록 수정하세요! (참고: tf.expand_dims(), np.newaxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "composite-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "regular-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.2\n",
    "vocab_size = SRC_VOCAB_SIZE\n",
    "\n",
    "transformer = Transformer(n_layers=n_layers,\n",
    "                         d_model=d_model,\n",
    "                         n_heads=n_heads,\n",
    "                         d_ff=d_ff,\n",
    "                         dropout=dropout,\n",
    "                         src_vocab_size=vocab_size,\n",
    "                         tgt_vocab_size=vocab_size,\n",
    "                         pos_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "brown-point",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-mainstream",
   "metadata": {},
   "source": [
    "### 10-4. 모델 밖의 조력자들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "outside-bachelor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "statistical-calcium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 208,
       "width": 426
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch, length = 16, 20\n",
    "src_padding = 5\n",
    "tgt_padding = 15\n",
    "\n",
    "src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = \\\n",
    "generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.set_title('1) Encoder Mask')\n",
    "ax2.set_title('2) Encoder-Decoder Mask')\n",
    "ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
    "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-employee",
   "metadata": {},
   "source": [
    "첫 번째 마스크는 각 배치 별로 데이터의 꼬리 부분을 Masking 하는 형태임을 알 수 있습니다. 낯선 부분은 두 번째와 세 번째의 Decoder가 연관된 마스크인데… 이것이 바로 Causality Mask와 Padding Mask를 결합한 형태입니다! 자기 회귀적인 특성을 살리기 위해 Masked Multi-Head Attention에서 인과 관계 마스킹을 했던 것을 기억하시죠? 인과 관계를 가리는 것도 중요하지만 Decoder 역시 <PAD> 토큰은 피해 가야 하기 때문에 이런 형태의 마스크가 사용된답니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-touch",
   "metadata": {},
   "source": [
    "### LearningRateSchedule 클래스를 상속받아 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "modular-halloween",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "organized-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "olive-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "royal-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dynamic-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='pre')\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fleet-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "painted-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fitting-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = ['오바마는 대통령이다.', '시민들은 도시 속에 산다.', '커피는 필요 없다.', '일곱 명의 사망자가 발생했다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "treated-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-delight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "mature-albany",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj62/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c9a2b13c134ca8a2c5cb8cd6ee0dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: they was they was a a of they of they of they of they of they of they of they of they of they of they of they of they of they .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they was ay of they of they of they of they of they of they of they of they of they of they of they of they of they of they .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: they was they was a of they . s . s . s . s . s . s . s . s . s . s . s . s . s . s . s . s . s . percent . .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: they was a a a of they of they of they of they of they of they of they of they of they of they of they of they of they of they .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40cb89e19e54709862db0c5ff418880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they have been they of they of they of they of they have been killed in the first times of they of they of they of they s around they s .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: they have they don t know they have been they of they of they have been they of they were not been they of they were not been in they of they\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death of the u . s . s . s . m . m . m . m . m . m . m . m . m . m . m . m . m . m . m . m . m . military\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e7f83348a1490abdccd7dc8eafcdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a campaign for the first of obama is a campaign is a campaign to be obama is a campaign for the first of the first of the first of the obama is leading to be obama to be obama to be a campaign . .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they were they were they of they were they were they in the first in the city of the city s . m . and they were in they were they were they in the city of .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: they don t know they don t know they don t know they don t know they don t think they don t think they don t think they don t think they don t think they .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the plane was killed in southern southern city of the southern city of the southern city s southern city of the southern city s . m . m . m . m . m . m . m . m . much of the city .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858a12e8fa1b4eecb4e0c7f46bb9a69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a campaigning campaigning campaigning campaigning campaign obama on tuesday on sunday as obama is a obama campaigning obama on sundayself obama on sunday in the obama campaigns of obamas that obama is obama on the obama campaign .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they ve learned they struck the two city of the two city struck the two mountain s . meters in the city s . ms in the city of the city s office is a small city s office in .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the coffee don t know the coffee don t know the coffee don t know the coffee don t know the coffee don t know the coffee don t know the coffee won t know the coffee but the coffee won t because they . .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: four four four four four people were killed in the four four four people were killed in the four four four four four people were killed in the town of the blasts were killed in the town of the blasts . .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8456b17c6be741d6be4af48d771a3322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama on the president of obama on the obama is a presidential campaigning on obama on obama on the president on obama on obama on obama on obama on the presidential nominations obama on the president obama on obama on obama on . .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they have been one of the great placed in the great places of the greatest s main place in the city of the city s main city of the city of the city of the greatest city in the city of .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: don t take anything there is no answer for anything in the coffee or no answers are no answers for the tables don t need to be given to be given up for but or but no answers or no coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: four four four four four people were killed in the four weeks and the four people were killed and wednesday s death toll on wednesday wednesday s top fire since wednesday s fourth weeks .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c488686302447b80db2ef5f3528dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is campaigning for the president who is campaigning for the president who is campaign for the president who is heading the president of the white house to head the president of the white house is heading out of the president of obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they walked in the mountain s streets and the mountain is heading on the mountain s the mountain s greatest in the mountain s peak peak peak in the mountain s main mountain in the beach on the mountain s peak peak beach .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee coffee coffee don t need coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee don t need coffee don t need coffee coffee don t need coffee you can t need you won t need coffee coffee coffee you won t be coffee coffee coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: all of the deadly were killed in wednesday night s deaths and wednesday and wednesday s deaths in the days and wednesday and wednesday s death toll in the days of .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db328a39fa3f43e6b947b728a2832bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the president elected president elect will be obama lead the president elect on february president elected on a former president of the lead to the lead only lead on a figure of the president elect on his president elect to obama . .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the mountain is a mountainried as man s great mountain mountain mountain mountain mountain mountain s man is a mountain mountain to be a mountainried in a mountain sl meters of the greatest mountain to be a mountain s great .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: needs to take to take the country to take on the other to take another country in the country to take on the table needs to take the need need to take on the other to need the need to take another country .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: nine people have died sunday in a crowd since sunday wednesday where a crowd chavez s death toll on sunday wednesday s death toll in a nationaliten since sunday s death toll since sunday were killed in a string .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cc2fe591d941bb98069cd48c721500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the president ll be in the first time when mccain hours race to be the president ll be a president ll be in the first time when mccain got the president to be president to the president of the president ll be in the first time .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they added to the crowds are almost always as far as far as far as med in the battle in which is the battle the mountain struck in which is the righted in the battle in the world s .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee coffee coffee any coffee but needs coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee any coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: two nine nine people have died in two battles since friday sunday were killed with two dozens of deaths since death with two dozens of deaths and wounded sunday were killed with fires at least seven people .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dbb8240e0c46d4a8e480809b469a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama on the white house is a former percentage campaign trail to be obama to the obama of the obama hours to be a number of obama hours obama to be presidential candidates obama to the number to be obama on television of .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the man s harbor was place in many s harbor city of many in many city where many city of many in city where a man has a placed in manyahoo s harbor in many people in city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: need to take place but the north is to take place but if you need to place to take place the north is need to place but need to place to place that important to north korea for the need to put the need to place coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death of the sunday s death was killed sunday after another narrow camp near the death of . another turkish s death was killed in the northern rocky s northern turkish deaths were killed in the blast .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bed83b7aa374de794a3b5e6774dcaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president obama said obama gave it should be president\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: it s learned many of many killings it s although places a crowded mante places and two places in places the always s place as many people have been placed in place .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it needs to take place for the coffee for the coffee for anything that it need only place on which is need on on sunday when you need need to coffee need to coffee for the coffee need to take another coffee . .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven civilian death toll wednesday gave another were killed nearly two people were killed in two neighborhoods on wednesday shooked two people dead were killed and two other people dead were killed in .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9d59f8c2b74b49b6bf2fcfecb70b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama who said he needed to be a member president who the member of the two member of the democratic presidential candidates who needed to be a member of the democratic presidential candidate who virtual during the eighth can thing .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: it s the crowd comes like many run as many as many as many as many as many as many as many as many as many as there are many as many opportunity .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: need to take her gloves to feel whether there were no needs needed need needs to take pace for questions of the need need need need need important to use important to send for it to rough for no needs .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: death sunday a third earthquake rattled sunday thursday sunday . sunday . sunday . roads in may have killed sunday thursday and four other deaths in the area on tuesday as death toll on tuesday as firefighters tuesday on tuesday in the area .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b0fa475e9040f2908d33945e5fd6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: he will needs to be the shot in the first president seem delegate but will need to visit the united nations president seems to place the democratic president res presidential visiting presidential candidates should be presidential to delegate .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: crowds believe crowds the country s place but in huge crowd an place crowded crowds that there are med crowded crowds in huge mountain and waves in italy believe a crowd of mancheing a crowded .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: take coffee for the coffee needs need to take the coffee for the coffee need to take coffee in the coffee for the u . need to take coffee or decst needs to take coffee for the coffee for the dec coffee needs .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: two third director of nine people sunday s seven were killed in sevened in two weeks and nine others have been killed in two weeks since sunday . have been killed in seven others and other sunday .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f5ce3ff1d6484f90e6003b7a6fe79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the presidential presidential candidate will presidential votes mccain member three presidential beak seems beak you to be president lee myung bak from his below the presidential presidential candidates will be president obama to all president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: many people in seoul and marked the crowd city of top city man city of man city city city of man city of man city of man city of man city of people are in two thirds of manado city in the city of .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: take on coffee for the coffee for the coffee is been much coffee for the coffee for the coffee for you need coffee and take coffee for your coffee is not coffee for the need to coffee for you need coffee or coffee coffee . .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: on sunday two others were killed in the deaths last minutes of a voteed in detail sunday sunday thursday and others seven other areas were killed in details of engineering sunday in seven others were reported .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f988a62e6f8346f8b11f3ecdf46006d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: i m proud of the president elect will president elect former president elect president elect barack obama of president elect barack obama being a president elected president elected president elect barack obama of mccain should be president to elect president a step on the president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: many of people are thought to places in place by many people are place . billion crowded in crowded crowds in mountain city of four people in place . man city of four people are thought to have be evacuated .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: take caffeine is but you burially whether you take coffee will take place in sales for coffee in coffee in korea december . or deca is whether you will take coffee in december . but you don t take coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: nine were officials have died in seven other deaths officials since friday s death of sunday were killed near the seven others were seven officials in the seven others were on monday s seven were killed seven week .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afe89c7b3bb4abf95b6de34dcf7cc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: she president seems fuller below of obama in the u . s . presidential visited his presidential visit down almost everybody i president a president who is presidential president so his aide for presidential presidential president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: many of two places are in miami where there was two places places in many place where there has two places in the place where there s places in two places and where huge places in their place\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: don t need to any care for the u . s . coffee need on penalty . coffee . don t care for any care whether coffee or but you need coffee need any care or that penalty or other penalty any care .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: nine were officials were injured sunday were reportedly hit with at the deaths and a hospital sunday night at the hospital sunday s streets sunday with at least people in the hospital sunday and others were injured sunday were reported .\n",
      "\n",
      "Hyperparameters\n",
      "n_layers : 2\n",
      "d_model : 512\n",
      "n_heads :  8\n",
      "d_ff : 2048\n",
      "dropout : 0.2\n",
      "\n",
      "TRAINING\n",
      "Warmup Steps: 4000\n",
      "Batch size : 64\n",
      "Epoch At :  15\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "    print('Translations' )\n",
    "    for sentence in sentences:\n",
    "        translate(sentence, transformer, ko_tokenizer, en_tokenizer)\n",
    "    print()\n",
    "    print('Hyperparameters' )\n",
    "    print('n_layers :', n_layers)\n",
    "    print('d_model :', d_model)\n",
    "    print('n_heads : ', n_heads)\n",
    "    print('d_ff :', d_ff)\n",
    "    print('dropout :', dropout)\n",
    "    print()\n",
    "    print('TRAINING')\n",
    "    print('Warmup Steps: 4000')\n",
    "    print('Batch size : 64')\n",
    "    print('Epoch At : ', epoch+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-brazil",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "Layer를 2개로 제한했기 때문인지 학습이 제대로 되지 않는 듯한 모습이었습니다. 모델을 단순히 이해하는 것이 아니라 구현한다는 것이 얼마나 어려운일인지 느낄 수 있었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-flood",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
